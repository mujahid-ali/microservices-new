Do not use this in prod: spring.jpa.hibernate.ddl-auto=create-drop
this can be used in prod: spring.jpa.hibernate.ddl-auto=none along with liquibase and flyway library.


Inter Service communication:
1: Synchronus
2: Asynchronus
1: sync -> via http: RestTemplate(by default from Springboot) and WebClient(From Spring webflux project) are two method
By default WebClient makes async request, for sync we need to use .block() method.
We will call the another service using WebClient using the uri of that service
EX: InventoryResponse[] inventoryResponses = webClient.get()
                    .uri("http://localhost:8082/api/inventory",
                            uriBuilder -> uriBuilder.queryParam("skuCode", skuCodes).build())
                    .retrieve()
                    .bodyToMono(InventoryResponse[].class)
                    .block();

Service Discovery(Eureka): We can not expect a fix URL for a particular micro-service bcz there can be multiple instance of
 a microservice and each instance can have a dynamic ip address. Service Discovery helps in this scenario. There should be
 a Discovery Server which should have all the information about the services(ex: service-name, ip-add, uri,ports, isUp etc).
 When a service initialized then it will try to register itself with Discovery server with all the info about it, this data is
 known as service registry.
 When Service A wants to communicate with Service B then first it will check with the Discovery server for Service B,
 the Discovery server will tell the info(ip,port) about Service B to Service A. Now A can use the info to communicate with B.
 Here we have avoided the hardcoding of urls of services by using discovery server.
 Discovery server also shared it's copy of service registry to services so in case Discovery Server is down then service
 can refer their own local copy of registry(it might not be latest but still serves purpose).

 https://spring.io/projects/spring-cloud-netflix Service Discovery: Eureka instances can be registered
 @EnableEurekaServer annotation will make the app/class as Eureka Discovery Server.
 @EnableEurekaClient annotation will make the service as discoverable for eureka server and
  adding the eureka server info in app.prop file will tell about the server it should add: eureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka

@LoadBalanced
WebClient.builder
If there are multiple instances of a service running then the calling service might not be able to figure out which instance to
invoke, so client side load balancing is used to resolve this issue.

If Discovery server is down then services will look into their local registry and make the calls, but if all the services are
down and then restart but discovery server is still down then they might not be able to communicate, in this case discovery is required.

API Gateway: Acts as an entry points and routes the user requests. Rules need to be configured for routing.
Routing based on Request Headers
Authentication
Security
Load Balancing: Decide which instance of service to call.
SSL Termination
Spring cloud gateway(spring framwork's api gateway) having some features like Security, rate-limiting, circuit-
breaker integration, path rewriting etc.
Doc for more details of predicate: https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gateway-request-predicates-factories


KeyCloak: docker run -p 8181:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:18.0.0 start-dev
We are using keycloak for securing our application. we can use above docker command to use it.
@EnableWebFluxSecurity is no mvc/web else @EnableWebSecurity to enable security.
configure method can be override for defining security rules.
Run command in terminal: docker run -p 8181:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:18.0.0 start-dev
go to :-> http://localhost:8181/ -> Administration Console -> admin/admin -> add realm -> spring-boot-microservices-realm -> create
-> clients -> create -> client ID: spring-cloud-client, protocol:openid connect -> save -> select access type: confidential,
disable: direct and flow, enable: Service Accounts Enabled: ON -> save -> Credentials -> Secret

Open postman:-> Authorization -> Type: OAuth2.0 ->
Access Token URL: http://localhost:8181/realms/spring-boot-microservices-realm/protocol/openid-connect/token
Client ID: spring-cloud-client
Secret: from 8181 server
Scope: openid offline_access
Get New Access Token
Use Token

Resilience4J Circuit Breaker: For resilient apps. fail fast and respond fast. Avoid cascading failure.
It is nothing maintaining set of stats in the app. Open -> Half open -> Closed -> Open.
Doc: https://resilience4j.readme.io/docs
SneakyThrows for catching the exceptions(not for prod env).
when Timeout and then some retry can be implemented.
Ex: Application.properties config
resilience4j.timelimiter.instances.inventory.timeout-duration=3s @TimeLimiter
resilience4j.retry.instances.inventory.max-attempts=3 @Retry
resilience4j.retry.instances.inventory.wait-duration=5s @Retry

http://localhost:8081/actuator/health to check circuit-breaker status
http://localhost:8081/actuator/
http://localhost:8081/actuator/retryevents
http://localhost:8081/actuator/timelimiterevents


Distributed Tracing:
It helps us to track the request from start to finish and help in debugging the issue.
Ex-> Why a request failed and where it failed ?
traceId is used to identify a request, spanId(number of trips a request is taking inside the system)
traceId unique identifier for whole request from start to end.
spanId unique identifier at every step or service(api-gateway -> span1, order -> span2)
So, a combination of both trace and span id is used to identify performance and issues for a request.

Sleuth and zipkin is used: https://zipkin.io/pages/quickstart.html
docker run -d -p 9411:9411 openzipkin/zipkin
http://localhost:9411/zipkin/

Debug logs with traceId and spanID: INFO [product-service,0e6782babf7a223b,ce94079375c18f4b]
We are calling inventory service in a new thread, so it's considered as a new request.
If we remove circuit breaker and Completable and rerun we would be able to see the complete path
from api-gateway -> order-service -> inventory-service in zipkin server.
To be able to trace for threads as well we need to enable/create own spans. Tracer from Sleuth.

Event Driven Architecture using Kafka:
Kafka is pull based approach(consumer will poll queue/partition).
RabbitMQ(RMQ) is push based approach(As soon as message comes to queue, queue will try to push it to consumer).
order-service is producer and notification-service is consumer
docker: https://developer.confluent.io/quickstart/kafka-docker/
docker-compose.yml
open terminal and go to microservices-new having docker-compose.yml and run
docker compose up -d
https://spring.io/projects/spring-kafka
https://docs.spring.io/spring-kafka/reference/html/
order-service will send notification and notification-service is subscribed to order-service.
order-service is producer: kafkaTemplate.send("notificationTopic", ) sending topic and
notification-service is consumer, consuming the topic defined in application.properties.

Components:->
Producer: Produces messages for a topic, if key is given then hash it find partition, if key or partition not given then select partition on round-robin basis.
Consumer: Consumes messages for a topic.
Consumer Group: Group of consumers.
Cluster: Group of brokers.
Broker: Kakfka-server or broker is started when we start kafka and it will communicate with producer. It can have multiple topics.  
Topic: A topic can have multiple partitions.
Partition: A Partition can have number of offsets. Data is residing in partition/Queue/Leader. Each partition is replicated to another broker for resiliency(follower).
Offset: The offset or commited_offset number will indicate messages consumes till now. Or it is a pointer pointing to next message that needs to be consumed.
Zookeeper: It manages the information and state about cluster, broker, topics, partitions and offset. If there is any partition/queue failure then the commited_offset number is already 
present in zookeper, so this state information is used by follower and follower will start servicing the messages.
Leader: Message read and write happens through leader.
Follower: Leader's data is replicated to follower, when leader is down then follower will take it's place and become leader, ensuring 
high availability even a queue/partition failed/down for some reason.
Dead letter Queue: Every message has some retry limit, if after retrying the threshold limit, the message is still failing or not consuming then it will be
sent to dead letter queue and commit_offset is increased to process next message.
Distributed messaging: Broker is having topic and topic is having partition. There can be multiple brokers serving same topic. Each partition should be copied to at least one more
broker serving the same topic. First broker having the topic/partition can be considered as Leader and another one as follower, in case of failure of leader, follower will take it's place.
If there is a topicA serverd by partition-0 and consumed by group1_consumer1 then the same group's consumer will not consume anything from partition-0 but group2_consumer1 can still consume messages 
from partition-0.
Advantages: Retry meachanism, async calls, overcoming speed mismatch between producer/consumer hence avoiding data loss, increases reliability.

RabbitMQ:-> 
Producer talks with an exchange, exchange is binded with a queue(via a routing key). Based on routing key the queue is decided.
Failed message is sent back to front of queue until all retry exhausted and then send it to dead letter queue.
Multiple types of exchanges: 
1: Fanout: push the message to all the associated queue(broadcast)
2: Direct: routing key is exactly matching with message key then it will rout to corresponding queue.
3: Topic Exchange: route based on wild card (*abc) or topic name.

Dockerize the application:
jib builds containers without using a Dockerfile or requiring a Docker installation: https://cloud.google.com/java/getting-started/jib


complete code: https://github.com/SaiUpadhyayula/spring-boot-microservices

Testing
--------------------------------------------------------------------------
Add dependency:
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <version>5.6.2</version>
            <scope>test</scope>
        </dependency>

 go to service.CommentService and pres ctr + shit + t and select method to generate test, make test class public.
 junit will provide -> assertions.assert

 a better choice is to use:-> assertj, assert-core which would provide
 assertThat and makes test more readable.

 Mockito:
When we need to test a functionality which is dependant on anther class then mockito is useful for mocking the
behaviour of other classes:
<dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-all</artifactId>
            <version>1.10.19</version>
            <scope>test</scope>
        </dependency>

So, either mock the dependencies or actually inject the dependencies.
mockito is providing us the dummy objects to continue testing functionality.

@ExtendWith(MockitoExtension.class)
@Mock
<dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>


 @BeforeEach can be used for instantiation and avoid duplication.
    public void setup()

Testing data access layer with testcontainers:
Embedded database like H2 can also be used but it has some limitations:
if we are mysql then using mysql for testing in ci env and maintaining that db is
a tedious process with maintainence overhead so testcontainers cab be use which provides
lightweight instance of common databases. It mainly uses docker in the background to create
containers.
@TestContainer
@Container
both are Junit5 annotations and this will force test container to create a new container for each test
Ex:-> For each test a new db-container will be created which is bad(slow, unmaitainable).
So make use of singleton pattern, so that only one db-container is created for all the test, hence improved performance.

MockMVC, Mockito
@WebMvcTest for controller/rest-api testing
@MockMvc provides servelet env for rest endpoints
@MockBean

